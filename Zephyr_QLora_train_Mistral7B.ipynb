{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Mistra use QLora on Zephyr medical dataset\n",
    "\n",
    "References: https://bdtechtalks.com/2023/11/03/gpt-llm-trainer/ & https://youtu.be/9bl1mJImj10?si=v9lazUoCWqc4d4Hq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "from accelerate import infer_auto_device_map\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1026dfb4b1414431812fb9078031c078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafefec0ca9948159fd4f2803b0531ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# this should be set as False for finetuning\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model=model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=max_seq_length,\n",
    "    padding_side=\"left\",\n",
    "    # add_eos_token=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.base_model.model.model.embed_tokens.weight.data = model.base_model.model.model.embed_tokens.weight.data.float()\n",
    "# model.base_model.model.lm_head.weight.data = model.base_model.model.lm_head.weight.data.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad_v2\")\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full, test_full = train_test_split(df, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_json(data):\n",
    "    # data = json.loads(json_data)\n",
    "    texts = data['text']\n",
    "    if len(texts) == 0:\n",
    "        return 'Unanswerable.'\n",
    "    text = texts[0]\n",
    "    if text[-1] != \".\":\n",
    "        text = text + \".\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['answer'] = train_full['answers'].apply(get_answer_from_json)\n",
    "test_full['answer'] = test_full['answers'].apply(get_answer_from_json)\n",
    "train = train_full.iloc[0:2000]\n",
    "test = test_full.iloc[0:200]\n",
    "eval_df = test_full.iloc[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_template(context, question, answer=\"\"):\n",
    "    s_part = f'Extract the answer to the question from the context. Answer with \"Unanswerable\" if the answer cannot be found.'\n",
    "    c_part = f\"### Context: {context}\"\n",
    "    q_part = f\"### Question: {question}\"\n",
    "    parts = [s_part, c_part, q_part]\n",
    "    a_part = f\"### Answer:\"\n",
    "    if answer:\n",
    "        a_part = a_part + \" \" + answer + \"\\n### End\"\n",
    "\n",
    "    parts.append(a_part)\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train)\n",
    "eval_ds = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "eos_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=max_seq_length,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "eos_tokenizer.pad_token = eos_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_tokenize_prompt(data_point):\n",
    "#     full_prompt = make_template(data_point[\"context\"], data_point[\"question\"], data_point[\"answer\"])\n",
    "#     result = eos_tokenizer(\n",
    "#         full_prompt,\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = train_ds.map(generate_and_tokenize_prompt)\n",
    "# tokenized_eval_dataset = eval_ds.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords_ids:list):\n",
    "        self.keywords = keywords_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0][-1] in self.keywords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# display(tokenizer.decode([13]))\n",
    "# '\\n'\n",
    "# display(tokenizer.decode([13, 27332]))\n",
    "# '\\n###'\n",
    "stop_ids = [13, tokenizer.eos_token_id]\n",
    "stop_criteria = KeywordsStoppingCriteria(stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "tokens_generated_count = 0\n",
    "token_generation_time = 0\n",
    "\n",
    "def get_clean_word_list(sentence):\n",
    "    words = sentence.split(\" \")\n",
    "    words = [re.sub('[^a-zA-Z0-9\\s]', '', x).lower() for x in words]\n",
    "    return words\n",
    "\n",
    "def score_answer(llm_answer, answer):\n",
    "    a_words = get_clean_word_list(answer)\n",
    "    a_words = set(a_words)\n",
    "    word_count = len(a_words)\n",
    "    l_words = get_clean_word_list(llm_answer)\n",
    "    count = 0\n",
    "    for word in l_words:\n",
    "        if word in a_words:\n",
    "            a_words.remove(word)\n",
    "            count += 1\n",
    "    score = float(count) / word_count\n",
    "    # arbitrary threshold\n",
    "    return int(score > 0.5)\n",
    "\n",
    "def get_redundancy(llm_answer, answer):\n",
    "    return len(llm_answer) / len(answer)\n",
    "\n",
    "def generate_answer(context, question, is_print=False):\n",
    "    global token_generation_time, tokens_generated_count\n",
    "    template = make_template(context, question)\n",
    "    if is_print:\n",
    "        print(template)\n",
    "    inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n",
    "    start = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
    "    )\n",
    "    token_generation_time += time.time() - start\n",
    "    tokens_generated_count += len(outputs[0]) - len(inputs[\"input_ids\"][0])\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text.removeprefix(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs01/miniconda3/envs/qlora/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the answer to the question from the context. Answer with \"Unanswerable\" if the answer cannot be found.\n",
      "### Context: Albania has often been called the 51st state for its perceived strongly pro-American positions, mainly because of the United States' policies towards it. In reference to President George W. Bush's 2007 European tour, Edi Rama, Tirana's mayor and leader of the opposition Socialists, said: \"Albania is for sure the most pro-American country in Europe, maybe even in the world ... Nowhere else can you find such respect and hospitality for the President of the United States. Even in Michigan, he wouldn't be as welcome.\" At the time of ex-Secretary of State James Baker's visit in 1992, there was even a move to hold a referendum declaring the country as the 51st American state. In addition to Albania, Kosovo which is predominately Albanian is seen as a 51st state due to the heavily presence and influence of the United States. The US has had troops and the largest base outside US territory, Camp Bondsteel in the territory since 1999.\n",
      "### Question: Who is the leader of the Socialists?\n",
      "### Answer: Edi Rama\n"
     ]
    }
   ],
   "source": [
    "row = test.iloc[4]\n",
    "text = make_template(row.context, row.question)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=512, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the answer to the question from the context. Answer with \"Unanswerable\" if the answer cannot be found.\n",
      "### Context: {context}\n",
      "### Question: {question}\n",
      "### Answer: {answer}\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "print(make_template(\"{context}\", \"{question}\", \"{answer}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:40<00:00,  1.70s/it]\n",
      "/tmp/ipykernel_323066/1904635009.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['base_llm_answer'] = test.progress_apply(lambda x: generate_answer(x.context, x.question).strip(), axis=1)\n",
      "/tmp/ipykernel_323066/1904635009.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['base_score'] = test.apply(lambda x: score_answer(x.base_llm_answer, x.answer), axis=1)\n",
      "/tmp/ipykernel_323066/1904635009.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['base_redundancy'] = test.apply(lambda x: get_redundancy(x.base_llm_answer, x.answer), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test['base_llm_answer'] = test.progress_apply(lambda x: generate_answer(x.context, x.question).strip(), axis=1)\n",
    "test['base_score'] = test.apply(lambda x: score_answer(x.base_llm_answer, x.answer), axis=1)\n",
    "test['base_redundancy'] = test.apply(lambda x: get_redundancy(x.base_llm_answer, x.answer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "base_score\n",
       "1    143\n",
       "0     57\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['base_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    143.000000\n",
       "mean       3.497930\n",
       "std        4.316378\n",
       "min        0.666667\n",
       "25%        0.923077\n",
       "50%        1.541667\n",
       "75%        4.641026\n",
       "max       21.142857\n",
       "Name: base_redundancy, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[test.base_score == 1]['base_redundancy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>base_llm_answer</th>\n",
       "      <th>base_score</th>\n",
       "      <th>base_redundancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Secondary education in the United States did not emerge ...</td>\n",
       "      <td>Who didn't benefit from secondary schools?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57815</th>\n",
       "      <td>A few special additions enhance the language-learning ex...</td>\n",
       "      <td>What is the largest and longest-running university-run f...</td>\n",
       "      <td>BYU's International Cinema.</td>\n",
       "      <td>BYU's International Cinema</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72886</th>\n",
       "      <td>When Eisenhower was elected President in 1952, he believ...</td>\n",
       "      <td>What was the main purpose of the Government Contract Com...</td>\n",
       "      <td>conducted surveys of the racial composition of federal e...</td>\n",
       "      <td>The main purpose of the Government Contract Committee wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.621053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46410</th>\n",
       "      <td>Nonetheless, within a few years of his death, Gregory of...</td>\n",
       "      <td>What do the Protestants call him?</td>\n",
       "      <td>Father of the Canon.</td>\n",
       "      <td>\"Father of the Canon\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107618</th>\n",
       "      <td>Albania has often been called the 51st state for its per...</td>\n",
       "      <td>Who is the leader of the Socialists?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Edi Rama</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69710</th>\n",
       "      <td>Ibn Sīnā wrote at least one treatise on alchemy, but sev...</td>\n",
       "      <td>According to some, what was Ibn Sina trying to do regard...</td>\n",
       "      <td>\"re-Aristotelianise\" Muslim philosophy.</td>\n",
       "      <td>According to some, Ibn Sina was trying to \"re-Aristoteli...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.487179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93911</th>\n",
       "      <td>The city was founded in 734 BC by the Phoenicians as Ziz...</td>\n",
       "      <td>Who founded Palermo in 734 AD?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>The Phoenicians founded Palermo in 734 BC, not in 734 AD.</td>\n",
       "      <td>0</td>\n",
       "      <td>4.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40196</th>\n",
       "      <td>In 1988, Apple sued Microsoft and Hewlett-Packard on the...</td>\n",
       "      <td>How long did the FSF boycott GUN software for the Macint...</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>The FSF boycotted GNU software for the Macintosh platfor...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84186</th>\n",
       "      <td>During Mubarak's presidency, Nasserist political parties...</td>\n",
       "      <td>What party came in third in the 2013 election?</td>\n",
       "      <td>Sabahi.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>0</td>\n",
       "      <td>1.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99763</th>\n",
       "      <td>The consensus among linguists is that modern, standard C...</td>\n",
       "      <td>How much has Slovak changed from the past until now?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69243</th>\n",
       "      <td>Maslow’s ‘‘Third Force Psychology Theory’’ even allows l...</td>\n",
       "      <td>Who says that D.H. Lawrence's \"pristine unconscious\" is ...</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23898</th>\n",
       "      <td>During the Partitions of Poland from 1772 to 1795, its m...</td>\n",
       "      <td>What did the Mach constitution do?</td>\n",
       "      <td>legal privileges of the szlachta were legally abolished.</td>\n",
       "      <td>The March Constitution of 1921 abolished the legal privi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33810</th>\n",
       "      <td>City and Guilds College was founded in 1876 from a meeti...</td>\n",
       "      <td>When was the City and Guilds College founded?</td>\n",
       "      <td>1876.</td>\n",
       "      <td>1876</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>Buddhism /ˈbudɪzəm/ is a nontheistic religion[note 1] or...</td>\n",
       "      <td>How do Buddhists believe their suffering can be ended?</td>\n",
       "      <td>through the direct understanding and perception of depen...</td>\n",
       "      <td>Buddhists believe that their suffering can be ended thro...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.525253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58690</th>\n",
       "      <td>Florida's nickname is the \"Sunshine State\", but severe w...</td>\n",
       "      <td>What is central Florida known as</td>\n",
       "      <td>lightning capital of the United States.</td>\n",
       "      <td>The lightning capital of the United States.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.102564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            context  \\\n",
       "99996   Secondary education in the United States did not emerge ...   \n",
       "57815   A few special additions enhance the language-learning ex...   \n",
       "72886   When Eisenhower was elected President in 1952, he believ...   \n",
       "46410   Nonetheless, within a few years of his death, Gregory of...   \n",
       "107618  Albania has often been called the 51st state for its per...   \n",
       "69710   Ibn Sīnā wrote at least one treatise on alchemy, but sev...   \n",
       "93911   The city was founded in 734 BC by the Phoenicians as Ziz...   \n",
       "40196   In 1988, Apple sued Microsoft and Hewlett-Packard on the...   \n",
       "84186   During Mubarak's presidency, Nasserist political parties...   \n",
       "99763   The consensus among linguists is that modern, standard C...   \n",
       "69243   Maslow’s ‘‘Third Force Psychology Theory’’ even allows l...   \n",
       "23898   During the Partitions of Poland from 1772 to 1795, its m...   \n",
       "33810   City and Guilds College was founded in 1876 from a meeti...   \n",
       "5150    Buddhism /ˈbudɪzəm/ is a nontheistic religion[note 1] or...   \n",
       "58690   Florida's nickname is the \"Sunshine State\", but severe w...   \n",
       "\n",
       "                                                           question  \\\n",
       "99996                    Who didn't benefit from secondary schools?   \n",
       "57815   What is the largest and longest-running university-run f...   \n",
       "72886   What was the main purpose of the Government Contract Com...   \n",
       "46410                             What do the Protestants call him?   \n",
       "107618                         Who is the leader of the Socialists?   \n",
       "69710   According to some, what was Ibn Sina trying to do regard...   \n",
       "93911                                Who founded Palermo in 734 AD?   \n",
       "40196   How long did the FSF boycott GUN software for the Macint...   \n",
       "84186                What party came in third in the 2013 election?   \n",
       "99763          How much has Slovak changed from the past until now?   \n",
       "69243   Who says that D.H. Lawrence's \"pristine unconscious\" is ...   \n",
       "23898                            What did the Mach constitution do?   \n",
       "33810                 When was the City and Guilds College founded?   \n",
       "5150         How do Buddhists believe their suffering can be ended?   \n",
       "58690                             What is central Florida known as    \n",
       "\n",
       "                                                             answer  \\\n",
       "99996                                                 Unanswerable.   \n",
       "57815                                   BYU's International Cinema.   \n",
       "72886   conducted surveys of the racial composition of federal e...   \n",
       "46410                                          Father of the Canon.   \n",
       "107618                                                Unanswerable.   \n",
       "69710                       \"re-Aristotelianise\" Muslim philosophy.   \n",
       "93911                                                 Unanswerable.   \n",
       "40196                                                 Unanswerable.   \n",
       "84186                                                       Sabahi.   \n",
       "99763                                                 Unanswerable.   \n",
       "69243                                                 Unanswerable.   \n",
       "23898      legal privileges of the szlachta were legally abolished.   \n",
       "33810                                                         1876.   \n",
       "5150    through the direct understanding and perception of depen...   \n",
       "58690                       lightning capital of the United States.   \n",
       "\n",
       "                                                    base_llm_answer  \\\n",
       "99996                                                  Unanswerable   \n",
       "57815                                    BYU's International Cinema   \n",
       "72886   The main purpose of the Government Contract Committee wa...   \n",
       "46410                                         \"Father of the Canon\"   \n",
       "107618                                                     Edi Rama   \n",
       "69710   According to some, Ibn Sina was trying to \"re-Aristoteli...   \n",
       "93911     The Phoenicians founded Palermo in 734 BC, not in 734 AD.   \n",
       "40196   The FSF boycotted GNU software for the Macintosh platfor...   \n",
       "84186                                                  Unanswerable   \n",
       "99763                                                  Unanswerable   \n",
       "69243                                                  Unanswerable   \n",
       "23898   The March Constitution of 1921 abolished the legal privi...   \n",
       "33810                                                          1876   \n",
       "5150    Buddhists believe that their suffering can be ended thro...   \n",
       "58690                   The lightning capital of the United States.   \n",
       "\n",
       "        base_score  base_redundancy  \n",
       "99996            1         0.923077  \n",
       "57815            1         0.962963  \n",
       "72886            1         1.621053  \n",
       "46410            1         1.050000  \n",
       "107618           0         0.615385  \n",
       "69710            1         2.487179  \n",
       "93911            0         4.384615  \n",
       "40196            0         5.692308  \n",
       "84186            0         1.714286  \n",
       "99763            1         0.923077  \n",
       "69243            1         0.923077  \n",
       "23898            1         1.928571  \n",
       "33810            1         0.800000  \n",
       "5150             1         1.525253  \n",
       "58690            1         1.102564  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtered_df = test[test['score'] == 0]\n",
    "with pd.option_context('display.max_colwidth', 60):\n",
    "    # display(filtered_df.head(10))\n",
    "    df1 = test[['context', 'question', \"answer\", \"base_llm_answer\", \"base_score\", \"base_redundancy\"]]\n",
    "    display(df1.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "\n",
    "from pathlib import Path  \n",
    "filepath = Path('base_model.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "df1 = test[['context', 'question', \"answer\", \"base_llm_answer\", \"base_score\", \"base_redundancy\"]]\n",
    "df1.to_csv(filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=16` and `lora_alpha=16` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                in_features=4096, out_features=1024, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                in_features=4096, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                in_features=4096, out_features=14336, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                in_features=4096, out_features=14336, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                in_features=14336, out_features=4096, bias=False\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=4096, out_features=32000, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the SFTTrainer from TRL library that gives a wrapper around transformers Trainer to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb, os\n",
    "# wandb.login()\n",
    "\n",
    "wandb_project = \"squad-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['context'])):\n",
    "        text = make_template(example['context'][i], example['question'][i], example['answer'][i])\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "response_template_with_context = \"\\n### Answer:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4d82cb5de446c7aa26e934e78b4b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bad3d60397482d938d95d8f92fd1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs01/miniconda3/envs/qlora/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:207: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "project = \"squad-finetune-2\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    # peft_config=peft_config,\n",
    "    # train_dataset=tokenized_train_dataset,\n",
    "    # eval_dataset=tokenized_eval_dataset,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=eos_tokenizer,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-4, # Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    packing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerhean1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cs01/stuff/qlora_training/wandb/run-20231010_103908-pfa0t288</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gerhean1/squad-finetune/runs/pfa0t288' target=\"_blank\">mistral-squad-finetune-2-2023-10-10-10-39</a></strong> to <a href='https://wandb.ai/gerhean1/squad-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gerhean1/squad-finetune' target=\"_blank\">https://wandb.ai/gerhean1/squad-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gerhean1/squad-finetune/runs/pfa0t288' target=\"_blank\">https://wandb.ai/gerhean1/squad-finetune/runs/pfa0t288</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 43:58, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.235642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.251745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.283176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.262872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>0.237007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.292642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.218049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.209315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.281108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.194502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.180340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.177903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.219673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.165944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.161434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.198997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.198191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.183238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.189435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.180912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.15756725454330445, metrics={'train_runtime': 2644.6431, 'train_samples_per_second': 3.025, 'train_steps_per_second': 0.378, 'total_flos': 9.495139258601472e+16, 'train_loss': 0.15756725454330445, 'epoch': 4.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099b50bd7d8b4f3da9449849c2bb2929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.unload()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"mistral-squad-finetune-2/checkpoint-700\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs01/miniconda3/envs/qlora/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the answer to the question from the context. Answer with \"Unanswerable\" if the answer cannot be found.\n",
      "### Context: When Eisenhower was elected President in 1952, he believed hiring practices and anti-discrimination laws should be decided by the states, although the administration gradually continued to desegregate the Armed Forces and the federal government.:50 The President also established the Government Contract Committee in 1953, which \"conducted surveys of the racial composition of federal employees and tax-supported contractors\".:50–51 The committee, chaired by Vice President Richard Nixon, had minimal outcomes in that they imposed the contractors with the primary responsibility of desegregation within their own companies and corporations.:51\n",
      "### Question: What was the main purpose of the Government Contract Committee?\n",
      "### Answer: to imposed the contractors with the primary responsibility of desegregation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = test.iloc[2]\n",
    "text = make_template(row.context, row.question)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id, stopping_criteria=StoppingCriteriaList([stop_criteria]))\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0]) - len(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [02:24<00:00,  1.38it/s]\n",
      "/tmp/ipykernel_333481/1200710657.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['llm_answer'] = test.progress_apply(lambda x: generate_answer(x.context, x.question).strip(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test['llm_answer'] = test.progress_apply(lambda x: generate_answer(x.context, x.question).strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_333481/1641050726.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['score'] = test.apply(lambda x: score_answer(x.llm_answer, x.answer), axis=1)\n",
      "/tmp/ipykernel_333481/1641050726.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['redundancy'] = test.apply(lambda x: get_redundancy(x.llm_answer, x.answer), axis=1)\n"
     ]
    }
   ],
   "source": [
    "test['score'] = test.apply(lambda x: score_answer(x.llm_answer, x.answer), axis=1)\n",
    "test['redundancy'] = test.apply(lambda x: get_redundancy(x.llm_answer, x.answer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>score</th>\n",
       "      <th>redundancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Secondary education in the United States did not emerge ...</td>\n",
       "      <td>Who didn't benefit from secondary schools?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57815</th>\n",
       "      <td>A few special additions enhance the language-learning ex...</td>\n",
       "      <td>What is the largest and longest-running university-run f...</td>\n",
       "      <td>BYU's International Cinema.</td>\n",
       "      <td>BYU's International Cinema.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72886</th>\n",
       "      <td>When Eisenhower was elected President in 1952, he believ...</td>\n",
       "      <td>What was the main purpose of the Government Contract Com...</td>\n",
       "      <td>conducted surveys of the racial composition of federal e...</td>\n",
       "      <td>to imposed the contractors with the primary responsibili...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46410</th>\n",
       "      <td>Nonetheless, within a few years of his death, Gregory of...</td>\n",
       "      <td>What do the Protestants call him?</td>\n",
       "      <td>Father of the Canon.</td>\n",
       "      <td>Father of the Canon.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107618</th>\n",
       "      <td>Albania has often been called the 51st state for its per...</td>\n",
       "      <td>Who is the leader of the Socialists?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Edi Rama.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69710</th>\n",
       "      <td>Ibn Sīnā wrote at least one treatise on alchemy, but sev...</td>\n",
       "      <td>According to some, what was Ibn Sina trying to do regard...</td>\n",
       "      <td>\"re-Aristotelianise\" Muslim philosophy.</td>\n",
       "      <td>\"under the name of the ancient Greek philosopher\".</td>\n",
       "      <td>0</td>\n",
       "      <td>1.282051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93911</th>\n",
       "      <td>The city was founded in 734 BC by the Phoenicians as Ziz...</td>\n",
       "      <td>Who founded Palermo in 734 AD?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Phoenicans.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40196</th>\n",
       "      <td>In 1988, Apple sued Microsoft and Hewlett-Packard on the...</td>\n",
       "      <td>How long did the FSF boycott GUN software for the Macint...</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84186</th>\n",
       "      <td>During Mubarak's presidency, Nasserist political parties...</td>\n",
       "      <td>What party came in third in the 2013 election?</td>\n",
       "      <td>Sabahi.</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99763</th>\n",
       "      <td>The consensus among linguists is that modern, standard C...</td>\n",
       "      <td>How much has Slovak changed from the past until now?</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69243</th>\n",
       "      <td>Maslow’s ‘‘Third Force Psychology Theory’’ even allows l...</td>\n",
       "      <td>Who says that D.H. Lawrence's \"pristine unconscious\" is ...</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23898</th>\n",
       "      <td>During the Partitions of Poland from 1772 to 1795, its m...</td>\n",
       "      <td>What did the Mach constitution do?</td>\n",
       "      <td>legal privileges of the szlachta were legally abolished.</td>\n",
       "      <td>Unanswerable.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.232143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33810</th>\n",
       "      <td>City and Guilds College was founded in 1876 from a meeti...</td>\n",
       "      <td>When was the City and Guilds College founded?</td>\n",
       "      <td>1876.</td>\n",
       "      <td>1876.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>Buddhism /ˈbudɪzəm/ is a nontheistic religion[note 1] or...</td>\n",
       "      <td>How do Buddhists believe their suffering can be ended?</td>\n",
       "      <td>through the direct understanding and perception of depen...</td>\n",
       "      <td>through the elimination of ignorance and craving.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.494949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58690</th>\n",
       "      <td>Florida's nickname is the \"Sunshine State\", but severe w...</td>\n",
       "      <td>What is central Florida known as</td>\n",
       "      <td>lightning capital of the United States.</td>\n",
       "      <td>the lightning capital of the United States.</td>\n",
       "      <td>1</td>\n",
       "      <td>1.102564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            context  \\\n",
       "99996   Secondary education in the United States did not emerge ...   \n",
       "57815   A few special additions enhance the language-learning ex...   \n",
       "72886   When Eisenhower was elected President in 1952, he believ...   \n",
       "46410   Nonetheless, within a few years of his death, Gregory of...   \n",
       "107618  Albania has often been called the 51st state for its per...   \n",
       "69710   Ibn Sīnā wrote at least one treatise on alchemy, but sev...   \n",
       "93911   The city was founded in 734 BC by the Phoenicians as Ziz...   \n",
       "40196   In 1988, Apple sued Microsoft and Hewlett-Packard on the...   \n",
       "84186   During Mubarak's presidency, Nasserist political parties...   \n",
       "99763   The consensus among linguists is that modern, standard C...   \n",
       "69243   Maslow’s ‘‘Third Force Psychology Theory’’ even allows l...   \n",
       "23898   During the Partitions of Poland from 1772 to 1795, its m...   \n",
       "33810   City and Guilds College was founded in 1876 from a meeti...   \n",
       "5150    Buddhism /ˈbudɪzəm/ is a nontheistic religion[note 1] or...   \n",
       "58690   Florida's nickname is the \"Sunshine State\", but severe w...   \n",
       "\n",
       "                                                           question  \\\n",
       "99996                    Who didn't benefit from secondary schools?   \n",
       "57815   What is the largest and longest-running university-run f...   \n",
       "72886   What was the main purpose of the Government Contract Com...   \n",
       "46410                             What do the Protestants call him?   \n",
       "107618                         Who is the leader of the Socialists?   \n",
       "69710   According to some, what was Ibn Sina trying to do regard...   \n",
       "93911                                Who founded Palermo in 734 AD?   \n",
       "40196   How long did the FSF boycott GUN software for the Macint...   \n",
       "84186                What party came in third in the 2013 election?   \n",
       "99763          How much has Slovak changed from the past until now?   \n",
       "69243   Who says that D.H. Lawrence's \"pristine unconscious\" is ...   \n",
       "23898                            What did the Mach constitution do?   \n",
       "33810                 When was the City and Guilds College founded?   \n",
       "5150         How do Buddhists believe their suffering can be ended?   \n",
       "58690                             What is central Florida known as    \n",
       "\n",
       "                                                             answer  \\\n",
       "99996                                                 Unanswerable.   \n",
       "57815                                   BYU's International Cinema.   \n",
       "72886   conducted surveys of the racial composition of federal e...   \n",
       "46410                                          Father of the Canon.   \n",
       "107618                                                Unanswerable.   \n",
       "69710                       \"re-Aristotelianise\" Muslim philosophy.   \n",
       "93911                                                 Unanswerable.   \n",
       "40196                                                 Unanswerable.   \n",
       "84186                                                       Sabahi.   \n",
       "99763                                                 Unanswerable.   \n",
       "69243                                                 Unanswerable.   \n",
       "23898      legal privileges of the szlachta were legally abolished.   \n",
       "33810                                                         1876.   \n",
       "5150    through the direct understanding and perception of depen...   \n",
       "58690                       lightning capital of the United States.   \n",
       "\n",
       "                                                         llm_answer  score  \\\n",
       "99996                                                 Unanswerable.      1   \n",
       "57815                                   BYU's International Cinema.      1   \n",
       "72886   to imposed the contractors with the primary responsibili...      0   \n",
       "46410                                          Father of the Canon.      1   \n",
       "107618                                                    Edi Rama.      0   \n",
       "69710            \"under the name of the ancient Greek philosopher\".      0   \n",
       "93911                                                   Phoenicans.      0   \n",
       "40196                                                 Unanswerable.      1   \n",
       "84186                                                 Unanswerable.      0   \n",
       "99763                                                 Unanswerable.      1   \n",
       "69243                                                 Unanswerable.      1   \n",
       "23898                                                 Unanswerable.      0   \n",
       "33810                                                         1876.      1   \n",
       "5150              through the elimination of ignorance and craving.      0   \n",
       "58690                   the lightning capital of the United States.      1   \n",
       "\n",
       "        redundancy  \n",
       "99996     1.000000  \n",
       "57815     1.000000  \n",
       "72886     0.800000  \n",
       "46410     1.000000  \n",
       "107618    0.692308  \n",
       "69710     1.282051  \n",
       "93911     0.846154  \n",
       "40196     1.000000  \n",
       "84186     1.857143  \n",
       "99763     1.000000  \n",
       "69243     1.000000  \n",
       "23898     0.232143  \n",
       "33810     1.000000  \n",
       "5150      0.494949  \n",
       "58690     1.102564  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtered_df = test[test['score'] == 0]\n",
    "with pd.option_context('display.max_colwidth', 60):\n",
    "    # display(filtered_df.head(10))\n",
    "    df1 = test[['context', 'question', \"answer\", \"llm_answer\", \"score\", \"redundancy\"]]\n",
    "    display(df1.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "1    143\n",
       "0     57\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    143.000000\n",
       "mean       1.057457\n",
       "std        0.338980\n",
       "min        0.552632\n",
       "25%        1.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        3.600000\n",
       "Name: redundancy, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[test.score == 1]['redundancy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.995458112085034\n"
     ]
    }
   ],
   "source": [
    "print(tokens_generated_count / token_generation_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47284"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_generated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.44584608078003"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_generation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "\n",
    "from pathlib import Path  \n",
    "filepath = Path('qlora_train1.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "df1 = test[['context', 'question', \"answer\", \"llm_answer\", \"score\", \"redundancy\"]]\n",
    "df1.to_csv(filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['is_different'] = False\n",
    "\n",
    "# Iterate over rows and compare the values in StringColumn1 and StringColumn2\n",
    "for index, row in test.iterrows():\n",
    "    if row['llm_answer'] != row['base_llm_answer']:\n",
    "        test.at[index, 'is_different'] = True\n",
    "    else:\n",
    "        test.at[index, 'is_different'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>base_llm_answer</th>\n",
       "      <th>llm_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40114</th>\n",
       "      <td>When was the Microsoft branded as Mac?</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable. The question is about the Macintosh brand,...</td>\n",
       "      <td>Unanswerable. The question is about the Macintosh comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     question        answer  \\\n",
       "40114  When was the Microsoft branded as Mac?  Unanswerable   \n",
       "\n",
       "                                                   base_llm_answer  \\\n",
       "40114  Unanswerable. The question is about the Macintosh brand,...   \n",
       "\n",
       "                                                        llm_answer  \n",
       "40114  Unanswerable. The question is about the Macintosh comput...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', 60):\n",
    "    # display(filtered_df.head(10))\n",
    "    df1 = test.loc[test.is_different]\n",
    "    df1 = df1[['question', \"answer\", \"base_llm_answer\", \"llm_answer\"]]\n",
    "    display(df1.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c16e689f4914882a5aa283aecb404fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"mistral-squad-finetune-2/checkpoint-700\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs01/miniconda3/envs/qlora/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:207: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/cs01/stuff/qlora_training/qlora_train1.ipynb Cell 75\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/cs01/stuff/qlora_training/qlora_train1.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m merged_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmerge_and_unload()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/cs01/stuff/qlora_training/qlora_train1.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m merged_model\u001b[39m.\u001b[39;49msave_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m./mistral-squad-model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/qlora/lib/python3.10/site-packages/transformers/modeling_utils.py:1953\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[39m# If the model has adapters attached, you can save the adapters\u001b[39;00m\n\u001b[1;32m   1952\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_loaded_in_4bit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _hf_peft_config_loaded:\n\u001b[0;32m-> 1953\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1954\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1955\u001b[0m     )\n\u001b[1;32m   1957\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msave_config\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m   1958\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1959\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1960\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported"
     ]
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./mistral-squad-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
